{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d5156b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num participants: 248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Analyze elicited soft label information \n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import itertools\n",
    "import json \n",
    "import ast\n",
    "import importlib \n",
    "import label_construction_utils as utils\n",
    "\n",
    "save_dir = \"./\"\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "most_prob_class_txt = \"Most Probable Class\"\n",
    "second_prob_class_txt = \"Second Most Probable Class\" \n",
    "imposs_txt = \"Impossible Class(es)\"\n",
    "\n",
    "most_prob_txt = f\"{most_prob_class_txt} Prob\"\n",
    "second_prob_txt = f\"{second_prob_class_txt} Prob\"\n",
    "\n",
    "none_option = \"No\" # from second \n",
    "\n",
    "def participant_completed(subj_df): \n",
    "    '''\n",
    "    Check if a participant completed the study\n",
    "    For ours, this is if they filled out the final instructions\n",
    "    Final survey page is the only one of type \"survey-text\"\n",
    "    So we can check if that page was reached \n",
    "    Return True if completed, else False\n",
    "    '''\n",
    "    final_survey_res = subj_df.loc[subj_df.trial_type == \"survey-text\"]\n",
    "    if len(final_survey_res) != 0: \n",
    "        return True\n",
    "    else: return False\n",
    "\n",
    "\n",
    "def process_response(data_entry, subj_id=None): \n",
    "    # return processed human response\n",
    "    # note: messy from to output format used for html inputs to avoid data saving issues\n",
    "    # extract dict directly from: https://stackoverflow.com/questions/988228/convert-a-string-representation-of-a-dictionary-to-a-dictionary\n",
    "    data_entry = ast.literal_eval(data_entry)\n",
    "    most_prob_class = data_entry[\"classSelect\"].split(\"mostProb\")[-1] # needed to store tag to avoid data overlap on saving\n",
    "    \n",
    "    # note: two errors came up in prob specification\n",
    "    # one person typed 8p, which we assume is 80\n",
    "    # a few annotators selected a most prob class, but wrote 0 for prob\n",
    "    # we convert this to 100, as any such annotator only did this once (we believe it is an annotation error)\n",
    "    # however, we note that we make these judgments in processing the annotations\n",
    "    # which are inherently noisy\n",
    "    if data_entry[\"prob\"] == \"8p\": most_prob = 80 # manually adjust\n",
    "    else: most_prob = float(data_entry[\"prob\"])\n",
    "    if most_prob == 0: most_prob = 100 \n",
    "        \n",
    "    second_most_prob_class = data_entry[\"classSelect2\"].split(\"secondProb\")[-1]\n",
    "    if \"prob2\" in data_entry and second_most_prob_class != none_option:\n",
    "        if data_entry[\"prob2\"] != \"\": second_prob = float(data_entry[\"prob2\"])\n",
    "        else:\n",
    "            second_prob = None\n",
    "    else: second_prob = None\n",
    "    \n",
    "    # all imposs classes selected had \"improbClassSelect\" as the starter tag, with class name after\n",
    "    save_tag = \"improbClassSelect\"\n",
    "    selected_imposs_classes = [save_txt.split(save_tag)[-1] for save_txt in set(data_entry.keys()) if save_tag in save_txt]\n",
    "    \n",
    "    return {most_prob_class_txt: most_prob_class, most_prob_txt: most_prob,\n",
    "            second_prob_class_txt: second_most_prob_class, second_prob_txt: second_prob,\n",
    "            imposs_txt: selected_imposs_classes} \n",
    "\n",
    "def annotator_accuracy(subj_df, use_top_2=False): \n",
    "    '''\n",
    "    Compute the accuracy of the class(es) selected as most probable against cifar10 \"gold\" labels\n",
    "    '''\n",
    "    score=0\n",
    "    subj_id = subj_df[id_col].iloc[0]\n",
    "    for response, filename in zip(subj_df.response, subj_df.filename): \n",
    "        response = process_response(response, subj_id)\n",
    "        eval_set, example_idx, cifar_label = filename.split(\".png\")[0].split(\"_\")[1:]\n",
    "        if cifar_label == response[most_prob_class_txt]: score += 1 \n",
    "        elif use_top_2 and cifar_label == response[second_prob_class_txt]: score+=1\n",
    "    return score/len(subj_df.response)\n",
    "    \n",
    "def clean_axes(ax):\n",
    "    # clean axes for display\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])\n",
    "    \n",
    "# note, classes are alphabetized, as per cifar-10h index matching \n",
    "class_names = ['Airplane', 'Automobile', 'Bird','Cat', 'Deer','Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "num_classes = len(class_names)\n",
    "class2idx = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "idx2class = {idx: class_name for class_name, idx in class2idx.items()}\n",
    "\n",
    "id_col = \"subject\"\n",
    "\n",
    "data_path = \"./raw_human_data.csv\"\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "'''\n",
    "Filter to just include main data \n",
    "'''\n",
    "response_df = df[(df.trial_type == 'survey-html-form')].reset_index()\n",
    "\n",
    "# store a mapping from image id to batch number (helping for figuring out which batches to investigate or run extra)\n",
    "image_id2batch = {}\n",
    "for image_id, batch_num in zip(response_df.img_id, response_df.condition): \n",
    "    image_id = int(image_id)\n",
    "    if image_id not in image_id2batch: image_id2batch[image_id] = batch_num\n",
    "        \n",
    "subj_ids = set(response_df[id_col])\n",
    "\n",
    "print(f\"Num participants: {len(subj_ids)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a439630",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Extract data from each annotator\n",
    "Save in a format ammenable to label creation \n",
    "'''\n",
    "\n",
    "filenames = set(response_df.filename)\n",
    "example_idxs = set([int(idx) for idx in set(response_df.img_id)])\n",
    "\n",
    "example_idx2filename = {int(filename.split(\"_\")[2]): filename for filename in filenames}\n",
    "\n",
    "# save all elicitation data per sample\n",
    "all_elicitation_per_example = {example_idx: [] for example_idx in example_idxs}  \n",
    "\n",
    "all_probs = []\n",
    "for subj_id in subj_ids: \n",
    "    subj_df = response_df.loc[response_df[id_col] == subj_id]\n",
    "    for (data_entry, filename, example_idx) in zip(subj_df.response, subj_df.filename, subj_df.img_id):\n",
    "                \n",
    "        elicited_data = process_response(data_entry, subj_id)\n",
    "        \n",
    "        all_elicitation_per_example[example_idx].append(elicited_data)\n",
    "        \n",
    "        most_prob = elicited_data[most_prob_txt]\n",
    "        second_prob = elicited_data[second_prob_txt]\n",
    "        all_probs.append(most_prob)\n",
    "        if second_prob is not None: all_probs.append(second_prob)\n",
    "\n",
    "\n",
    "with open(\"human_soft_labels_data.json\", \"w\") as f:\n",
    "    json.dump(all_elicitation_per_example, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce58e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Some additional analyses and stats on elicited info\n",
    "'''\n",
    "\n",
    "# Compute annotator accuracy against CIFAR-10 labels\n",
    "accs = []\n",
    "top2_accs = []\n",
    "for subj_id in subj_ids: \n",
    "    subj_df = response_df.loc[response_df[id_col] == subj_id]\n",
    "    acc = annotator_accuracy(subj_df)\n",
    "    acc2 = annotator_accuracy(subj_df, use_top_2 = True)\n",
    "    accs.append(acc)\n",
    "    top2_accs.append(acc2)\n",
    "    \n",
    "print(f\"Accuracy of annotators' Top 1 Most Prob pred: {round(np.mean(accs), 3)*100}%\")\n",
    "print(f\"Accuracy of annotators' combined Top 1 and Top 2 Most prob preds: {round(np.mean(top2_accs)*100, 3)}%\")\n",
    "\n",
    "# Compute elicitation time \n",
    "sec_per_img = np.mean(response_df.rt) / (1000)\n",
    "print(f\"Avg seconds per image: {round(sec_per_img,2)} sec\")\n",
    "med_sec_per_img = np.median(response_df.rt) / (1000)\n",
    "print(f\"Median seconds per image: {round(med_sec_per_img,2)} sec\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
